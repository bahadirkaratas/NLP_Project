{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "888c91ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c47a7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('translated_chatgpt.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "df_ai = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "69bf7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(\"trnews-64/trnews-64.train.raw\") as fi:\n",
    "    articles = re.split(\"\\n\\n\", fi.read()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e24b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a71c1131",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated = []\n",
    "for article in articles:\n",
    "    splitted = article.split(\" \")\n",
    "    truncated.append(splitted[0:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "420cdd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_text_human = []\n",
    "for trunc in truncated:\n",
    "    truncated_text_human.append(\" \".join(trunc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7fbbec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRUNCATED HUMAN 44 ve AI 999 KARŞILIĞI YOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5452b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human = pd.DataFrame(truncated_text_human, columns = ['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24e51849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human = df_human.drop(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3387f1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Duruşmanın ilk duruşmalarında, Roj TV ile ilgili bir kararın Kasım ayında yapılması tahmin edildi, ancak tanık sayısı arttı ve belgelendirme zamanı geldiğinde, kararın Aralık ayında ilan edilmesi bekleniyor. Soruşturma ve deneme Kopenhag Şehir Mahkemesi, Danimarka\\'da devam etmektedir. Son duruşmada, soruşturmaya liderlik eden Bölgesel Şef Savcı Veers Riisager tarafından hazırlanan belgeler, mahkeme için bir televizyon ekranında gözden geçirildi. Belgeler, Belçika\\'daki ROJ NV stüdyolarında polis baskınları sırasında ele alındı, Roj TV için programları üretti. Belgeler, Roj TV\\'nin yayın politikası hakkında terör örgütü önerileri içeriyordu ve bu da organizasyonun lideri Abdullah Öcalan ve yayınlarda 30 yıllık çatışmayı içeriyordu. 25 sayfalık belge \" Yayın Politikaları ile ilgili kararsızlar\" olarak adlandırıldı. Önceki duruşmalarda savcı, Türkiye\\'deki askeri, polis ve sivillerin yaptığı saldırıları vurguladı. Ayrıca Eylül ve Ekim aylarında organize edilen terör saldırıları hakkında kapsamlı bilgi verdi. Daha önce deneme, ilerlemelerin önemli bir kısmının Roj TV\\'ye gittiğini ortaya çıkardı.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ai.iloc[44]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d56c3f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34bfe4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ai['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e58e5b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df_ai, df_human])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b336103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['Text'], data['label'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f4ada9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_test[0:330]\n",
    "y_test = y_test[330:]\n",
    "X_val = X_test[0:330]\n",
    "X_test = X_test[330:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e53eb25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-turkish-128k-uncased')\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(X_val.tolist(), truncation=True, padding=True)\n",
    "\n",
    "train_labels = y_train.tolist()\n",
    "val_labels = y_val.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "89d6eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(train_encodings['input_ids']),\n",
    "        torch.tensor(train_encodings['attention_mask']),\n",
    "        torch.tensor(train_labels))\n",
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(val_encodings['input_ids']),\n",
    "        torch.tensor(val_encodings['attention_mask']),\n",
    "        torch.tensor(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "888383d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/bahadir/Library/Python/3.9/lib/python/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1340\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 252\n",
      "  Number of trainable parameters = 184346882\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 1:10:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.391900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=252, training_loss=0.4562872477940151, metrics={'train_runtime': 4256.9585, 'train_samples_per_second': 0.944, 'train_steps_per_second': 0.059, 'total_flos': 1057706442547200.0, 'train_loss': 0.4562872477940151, 'epoch': 3.0})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('dbmdz/bert-base-turkish-128k-cased', num_labels=2)\n",
    "training_args= TrainingArguments(\n",
    "    output_dir=f'./results_bert-base-turkish-128k-cased/fold/english',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f'./logs_bert-base-turkish-128k-cased/fold/english',\n",
    "    logging_steps=100)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=lambda data: {'input_ids': torch.stack([item[0] for item in data]),\n",
    "                                'attention_mask': torch.stack([item[1] for item in data]),\n",
    "                                'labels': torch.stack([item[2] for item in data])})\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7fd64fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model, 'model_1.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ca3cb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 331\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)\n",
    "num_test_samples = len(test_encodings['input_ids'])\n",
    "test_labels = [0] * num_test_samples\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(test_encodings['input_ids']),\n",
    "    torch.tensor(test_encodings['attention_mask']),\n",
    "    torch.tensor(test_labels))\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aeee6737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.92       162\n",
      "           1       0.88      0.98      0.93       169\n",
      "\n",
      "    accuracy                           0.92       331\n",
      "   macro avg       0.93      0.92      0.92       331\n",
      "weighted avg       0.93      0.92      0.92       331\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d5d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
